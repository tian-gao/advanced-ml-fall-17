{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial: a simple logistic regression example\n",
    "\n",
    "In this notebook, we will implement a simple logistic regression example using tensorflow. We will apply the basic concepts of tensorflow to implement logistic regression using gradient descent on the mnist dataset. The mnist handwritten digit recognition dataset is a very classical dataset. It presents no challenge today, and best known algorithms can reach an accuracy of 99+%. However, it can still be instructive as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's import the data and have a look first\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist images are 28 x 28 grayscale images, and hence can be represented as vectors of length 784. Modern image algorithms work on much larger images, and usually 3 colour channels (rgb). The training dataset will thus often be represented as a 4-dimensional tensor (sample, height, width, channel), or (sample, channel, heigth, width). In this example, we will simply use the images as a long vector, and our dateset will thus be represented as a 2-dimensional tensor (sample, pixel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADb1JREFUeJzt3X+IVXUax/HPk1aQBWWhmdnalixbweY2RD+2xbCkFkEL\nCqUWlxVHzGCFhZL+KSgplmp3+6NwpCmjHxbYD7PYConapS2ckSjNdYyYatZJNworqKR89o857k42\n93vu3HvOPXfmeb9A5t7z3HPO020+c86958fX3F0A4jmi6gYAVIPwA0ERfiAowg8ERfiBoAg/EBTh\nB4Ii/EBQhB8IamIrV2ZmnE4IlMzdrZ7XNbXlN7MrzGyXmb1vZqubWRaA1rJGz+03swmS+iRdLmlA\n0lZJi939vcQ8bPmBkrViy3++pPfd/QN3PyBpg6QFTSwPQAs1E/7pkj4e9nwgm/YDZtZpZj1m1tPE\nugAUrJkv/EbatfjRbr27d0nqktjtB9pJM1v+AUkzhj0/VdKe5toB0CrNhH+rpFlmdrqZHSVpkaRN\nxbQFoGwN7/a7+3dmdqOklyRNkNTt7jsK6wxAqRo+1NfQyvjMD5SuJSf5ABi7CD8QFOEHgiL8QFCE\nHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqlt+4GRuPEE09M1u+8885kfenSpTVrEyZM\naKin8YQtPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExXF+VGbevHnJ+rp165L16dN/NDrcD6TuTH3e\neecl5+3t7U3WxwO2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFOj9JpZv6QvJX0v6Tt378h5PaP0\njjNz585N1m+66aaatcsuuyw5b5kjSE+cOH5Pcal3lN4i3oFL3f3TApYDoIXY7QeCajb8LullM+s1\ns84iGgLQGs3u9l/s7nvMbIqkV8zsX+7++vAXZH8U+MMAtJmmtvzuvif7uU/SM5LOH+E1Xe7ekfdl\nIIDWajj8ZjbJzI479FjSPEnbi2oMQLma2e2fKukZMzu0nMfd/W+FdAWgdA2H390/kPSLAntBG5o/\nf36yft999yXrp512WpHt/EBfX1+yfvXVV5e27vGAQ31AUIQfCIrwA0ERfiAowg8ERfiBoJq6pHfU\nK+OS3rZz/PHHJ+tvvvlmsj5r1qyG171r165kPe/W3Rs2bEjWBwcHR93TeFDvJb1s+YGgCD8QFOEH\ngiL8QFCEHwiK8ANBEX4gqPF7/2JIkk455ZRkfe3atcn6mWeemaznnSeSOla/YsWK5LwoF1t+ICjC\nDwRF+IGgCD8QFOEHgiL8QFCEHwiK6/nHufvvvz9ZX758ebLe7O/HzJkza9YGBgaaWjZGxvX8AJII\nPxAU4QeCIvxAUIQfCIrwA0ERfiCo3Ov5zaxb0nxJ+9z9nGzaZElPSpopqV/Ste7+eXltIuXZZ5+t\nWbvyyiubWvYLL7yQrN9+++3JetR7548F9Wz5H5Z0xWHTVkva4u6zJG3JngMYQ3LD7+6vS/rssMkL\nJK3PHq+XtLDgvgCUrNHP/FPdfVCSsp9TimsJQCuUfg8/M+uU1Fn2egCMTqNb/r1mNk2Ssp/7ar3Q\n3bvcvcPdOxpcF4ASNBr+TZKWZI+XSHqumHYAtEpu+M3sCUn/lPQzMxsws6WS7pJ0uZntlnR59hzA\nGML1/GPA6tXpI6lr1qxpeNm7du1K1hcuTB/I6evra3jdKAfX8wNIIvxAUIQfCIrwA0ERfiAowg8E\nxRDdbeDoo49O1ufOnZuspw7X5l1Se9ZZZyXrZbrgggtKXf4nn3xSs9bf31/quscCtvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBTH+dvARRddlKxfeumlDS/7jjvuaHjeesyYMSNZX7FiRc3azTffnJy3\n2cvNU+c45PUdAVt+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiKW3e3gVdffTVZv+SSS5L11O2zL7zw\nwuS8+/fvT9aXLVuWrOfdNnzy5Mk1a2bpO0yX+bs5ceL4PcWFW3cDSCL8QFCEHwiK8ANBEX4gKMIP\nBEX4gaByD3aaWbek+ZL2ufs52bTbJC2T9J/sZbe4+4tlNTnezZkzJ1k/ePBgsv7GG2/UrE2aNCk5\n75YtW5L12bNnJ+vNyPvv+uabb5L1Y445psh2wqlny/+wpCtGmP5ndz83+0fwgTEmN/zu/rqkz1rQ\nC4AWauYz/41m9o6ZdZvZCYV1BKAlGg3/A5LOkHSupEFJ99R6oZl1mlmPmfU0uC4AJWgo/O6+192/\nd/eDktZJOj/x2i5373D3jkabBFC8hsJvZtOGPb1K0vZi2gHQKvUc6ntC0hxJJ5nZgKRbJc0xs3Ml\nuaR+SctL7BFACXLD7+6LR5j8YAm9hNXTk/46JO9Y+7fffluz1tvbm5x3ypQpyXqz19Rv27atZu35\n559Pzvvhhx8m693d3Q31JEnz589P1jdv3tzwsscKzvADgiL8QFCEHwiK8ANBEX4gKMIPBMWtu1tg\n8eKRjpb+36OPPpqsl/n/KO/22Rs3bkzW77333mQ9dajvuuuuS867atWqZP3ss89O1lPyhuhODe/d\n7rh1N4Akwg8ERfiBoAg/EBThB4Ii/EBQhB8IavyOU9xGTj755KpbqOm1115L1vOG4P7666+T9ZUr\nV9as3X333cl5mz2/4YYbbqhZG8vH8YvClh8IivADQRF+ICjCDwRF+IGgCD8QFOEHguI4fxs44oj0\n3+C8oaxTUtfTS+lj4ZK0bNmyZH3RokXJeuoch7x7CRw4cCBZf+ihh5L1rq6uZD06tvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EFTucX4zmyHpEUknSzooqcvd/2pmkyU9KWmmpH5J17r75+W1On7lHcdv\n5rr2vOG9d+zYUdq68+bPO46fd9/+tWvXNtQThtSz5f9O0h/d/eeSLpC00szOkrRa0hZ3nyVpS/Yc\nwBiRG353H3T3bdnjLyXtlDRd0gJJ67OXrZe0sKwmARRvVJ/5zWympNmS3pI01d0HpaE/EJKmFN0c\ngPLUfW6/mR0raaOkVe7+Rd552cPm65TU2Vh7AMpS15bfzI7UUPAfc/ens8l7zWxaVp8mad9I87p7\nl7t3uHtHEQ0DKEZu+G1oE/+gpJ3uPnxI1k2SlmSPl0h6rvj2AJSlnt3+iyX9VtK7ZvZ2Nu0WSXdJ\nesrMlkr6SNI15bQ49u3evbvqFirT19dXs3b99dcn5827HBnNyQ2/u/9DUq0P+HOLbQdAq3CGHxAU\n4QeCIvxAUIQfCIrwA0ERfiAoa/aSzVGtzKx1KxtD8i5NXbp0aWnrzjtNe/Pmzcn6iy++mKxv2LCh\nZm3//v3JedEYd6/r3Hu2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFMf5gXGG4/wAkgg/EBThB4Ii\n/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqNzwm9kMM3vVzHaa2Q4z\n+0M2/TYz+7eZvZ39+0357QIoSu7NPMxsmqRp7r7NzI6T1CtpoaRrJX3l7nfXvTJu5gGUrt6beUys\nY0GDkgazx1+a2U5J05trD0DVRvWZ38xmSpot6a1s0o1m9o6ZdZvZCTXm6TSzHjPraapTAIWq+x5+\nZnaspNckrXH3p81sqqRPJbmk2zX00eD3Octgtx8oWb27/XWF38yOlLRZ0kvufu8I9ZmSNrv7OTnL\nIfxAyQq7gacNDeP6oKSdw4OffRF4yFWSto+2SQDVqefb/l9J+rukdyUdzCbfImmxpHM1tNvfL2l5\n9uVgalls+YGSFbrbXxTCD5SP+/YDSCL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQf\nCIrwA0ERfiAowg8ElXsDz4J9KunDYc9Pyqa1o3btrV37kuitUUX29pN6X9jS6/l/tHKzHnfvqKyB\nhHbtrV37kuitUVX1xm4/EBThB4KqOvxdFa8/pV17a9e+JHprVCW9VfqZH0B1qt7yA6hIJeE3syvM\nbJeZvW9mq6vooRYz6zezd7ORhysdYiwbBm2fmW0fNm2ymb1iZruznyMOk1ZRb20xcnNiZOlK37t2\nG/G65bv9ZjZBUp+kyyUNSNoqabG7v9fSRmows35JHe5e+TFhM/u1pK8kPXJoNCQz+5Okz9z9ruwP\n5wnufnOb9HabRjlyc0m91RpZ+neq8L0rcsTrIlSx5T9f0vvu/oG7H5C0QdKCCvpoe+7+uqTPDpu8\nQNL67PF6Df3ytFyN3tqCuw+6+7bs8ZeSDo0sXel7l+irElWEf7qkj4c9H1B7Dfntkl42s14z66y6\nmRFMPTQyUvZzSsX9HC535OZWOmxk6bZ57xoZ8bpoVYR/pNFE2umQw8Xu/ktJV0pame3eoj4PSDpD\nQ8O4DUq6p8pmspGlN0pa5e5fVNnLcCP0Vcn7VkX4ByTNGPb8VEl7KuhjRO6+J/u5T9IzGvqY0k72\nHhokNfu5r+J+/sfd97r79+5+UNI6VfjeZSNLb5T0mLs/nU2u/L0bqa+q3rcqwr9V0iwzO93MjpK0\nSNKmCvr4ETOblH0RIzObJGme2m/04U2SlmSPl0h6rsJefqBdRm6uNbK0Kn7v2m3E60pO8skOZfxF\n0gRJ3e6+puVNjMDMfqqhrb00dMXj41X2ZmZPSJqjoau+9kq6VdKzkp6SdJqkjyRd4+4t/+KtRm9z\nNMqRm0vqrdbI0m+pwveuyBGvC+mHM/yAmDjDDwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUP8F\nrt03IR9fW7sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c05249390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mnist.train.images[121].shape)\n",
    "plt.imshow(mnist.train.images[121].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "One-hot encoding is a commonly used form to encode the labels. It transform the label into a vector $(0, 0, \\dotsc, 1, \\dotsc, 0)$, where the $1$ is at the index corresponding to the label. In our case, this transforms the vector of labels into a matrix, with each row corresponding to a one-hot encoded label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "We quickly recall multiclass logistic regression. In multiclass logistic regression, we output a linear predictor for each possible outcome class, and then compute the respective probabilities as the softmax. To obtain the estimator, we simply minimize the cross-entropy loss with the true data. We will use gradient descent to do so.\n",
    "\n",
    "## Gradient descent in Tensorflow\n",
    "\n",
    "One of the main features of tensorflow is the possibility to automatically compute gradients (by using the back-propagation rule you learned in class). This means that tensorflow can automate the implementation of gradient descent for us. We will be using an automatically provided gradient for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you get errors everywhere, your graph has gone slightly haywire.\n",
    "# Just run this cell to reset everything\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model and the training setup\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # the image\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # the one-hot label\n",
    "\n",
    "W = tf.get_variable('W', shape=[784, 10])\n",
    "b = tf.get_variable('b', shape=[10])\n",
    "\n",
    "logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "\n",
    "# Compute the average cross-entropy across all the examples.\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "train_step = opt.minimize(cross_entropy)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some operations to measure accuracy\n",
    "\n",
    "prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "true_label = tf.argmax(y, 1, name='true_class')\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: accuracy is 0.08940000087022781\n",
      "Step 101: accuracy is 0.9077997207641602\n",
      "Step 201: accuracy is 0.9185997247695923\n",
      "Step 301: accuracy is 0.9275997877120972\n",
      "Step 401: accuracy is 0.9337997436523438\n",
      "Step 501: accuracy is 0.9365997910499573\n",
      "Step 601: accuracy is 0.9395997524261475\n",
      "Step 701: accuracy is 0.9433997273445129\n",
      "Step 801: accuracy is 0.9463996887207031\n",
      "Step 901: accuracy is 0.9475997090339661\n",
      "Accuracy on test set: 0.9052000045776367\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        _, ac = sess.run((train_step, accuracy), feed_dict={x: mnist.train.images[:5000,:], y: mnist.train.labels[:5000, :]})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Step {0}: accuracy is {1}'.format(i + 1, ac))\n",
    "    \n",
    "    ac = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Accuracy on test set: {0}'.format(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it better: tensorflow code tips\n",
    "\n",
    "Although the code above is perfectly functional, it has numerous issues in terms of maintanability, and is very prone to errors (for example, we can only run the cell once). Below, we explore some tensorflow features and code guidelines to make tensorflow code more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is often advantageous to define the model parts in a function\n",
    "# This makes them reusable. More complex models will often be composed\n",
    "# of similar parts stuck together.\n",
    "\n",
    "def compute_logits(x):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    W = tf.get_variable('W', shape=[784, 10])\n",
    "    b = tf.get_variable('b', shape=[10])\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Note: this function is implemented in tensorflow as\n",
    "# tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# We have included it here for illustration only, please don't use it.\n",
    "def compute_cross_entropy(logits, y):\n",
    "    y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also define our training loop as a separate function.\n",
    "\n",
    "def train_model(train_step, accuracy, inputs):\n",
    "    x, y = inputs\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(1000):\n",
    "            _, ac = sess.run((train_step, accuracy), feed_dict={x: mnist.train.images[:5000,:], y: mnist.train.labels[:5000, :]})\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Step {0}: accuracy is {1}'.format(i + 1, ac))\n",
    "\n",
    "        ac = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print('Accuracy on test set: {0}'.format(ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: accuracy is 0.12759998440742493\n",
      "Step 101: accuracy is 0.8617997169494629\n",
      "Step 201: accuracy is 0.9401997327804565\n",
      "Step 301: accuracy is 0.9455996751785278\n",
      "Step 401: accuracy is 0.9555997848510742\n",
      "Step 501: accuracy is 0.9695996642112732\n",
      "Step 601: accuracy is 0.9681997299194336\n",
      "Step 701: accuracy is 0.9745997786521912\n",
      "Step 801: accuracy is 0.9757997393608093\n",
      "Step 901: accuracy is 0.9719997644424438\n",
      "Accuracy on test set: 0.8773000240325928\n",
      "Done in 17.55894879448323 seconds\n"
     ]
    }
   ],
   "source": [
    "# As we mentioned in the introduction tutorial, it is better\n",
    "# to be explicit about the default graph\n",
    "\n",
    "# We put everything together in here.\n",
    "import time\n",
    "\n",
    "t_begin = time.perf_counter()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    logits = compute_logits(x)\n",
    "    loss = compute_cross_entropy(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train_step = opt.minimize(loss)\n",
    "    \n",
    "    train_model(train_step, accuracy, (x, y))\n",
    "\n",
    "t_end = time.perf_counter()\n",
    "print('Done in {0} seconds'.format(t_end - t_begin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

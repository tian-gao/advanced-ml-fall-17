{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial: tensorboard and other best practices\n",
    "\n",
    "Unfortunately, tensorflow is full of footguns (ways of shooting yourself in the foot). By following some best practices, we can reduce substantially the amount of frustration we have when working with tensorflow. Unlike other deep learning frameworks, tensorflow includes an extremely useful tool, called tensorboard. It allows you to visualize the graph, and the progress of the training.\n",
    "\n",
    "We will start from our example of logistic regression and add all the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start with our existing model code\n",
    "\n",
    "def compute_logits(x):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    W = tf.get_variable('W', shape=[784, 10])\n",
    "    b = tf.get_variable('b', shape=[10])\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Note: this function is implemented in tensorflow as\n",
    "# tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# We have included it here for illustration only, please don't use it.\n",
    "def compute_cross_entropy(logits, y):\n",
    "    y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Of course, we also need to load the data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "To display data in tensorboard, we must ask tensorflow to record the data. This data is recorded through summaries. There are two main types of summaries: scalars and histograms. Scalars summarise a single number, often the loss, or the learning rate. Histograms allow us to summaries the distribution of a set of data (usually a tensor, or its gradient). Tensorflow is also able to produce summaries for the input data, such as images or audio.\n",
    "\n",
    "These must be evaluated during the training step, and written out to a file using `tf.summary.FileWriter`.\n",
    "\n",
    "To visualize this information, we need to start an external program, called tensorboard. This is installed with tensorflow. From the command line, make sure to activate your tensorflow environment. You may then call\n",
    "```bash\n",
    "tensorboard --logdir=/path/to/log/directory\n",
    "```\n",
    "to start tensorboard. You can then connect using your browser at http://localhost:6006/.\n",
    "\n",
    "In our example here, if you place yourself in the directory of this file, you should run\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: accuracy is 0.06659999489784241\n",
      "Step 11: accuracy is 0.8277997970581055\n",
      "Step 21: accuracy is 0.8613997101783752\n",
      "Step 31: accuracy is 0.8735997676849365\n",
      "Step 41: accuracy is 0.8823997378349304\n",
      "Step 51: accuracy is 0.8887996673583984\n",
      "Step 61: accuracy is 0.893399715423584\n",
      "Step 71: accuracy is 0.8961997628211975\n",
      "Step 81: accuracy is 0.8989997506141663\n",
      "Step 91: accuracy is 0.9015997648239136\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    logits = compute_logits(x)\n",
    "    loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train_step = opt.minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Let's put the summaries below\n",
    "    \n",
    "    # create summary for loss and accuracy\n",
    "    tf.summary.scalar('loss', loss) \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # create summary for logits\n",
    "    tf.summary.histogram('logits', logits)\n",
    "    \n",
    "    # create summary for input image\n",
    "    tf.summary.image('input', tf.reshape(x, [-1, 28, 28, 1]))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter('logs/example1', sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(100):\n",
    "            _, ac, summary = sess.run((train_step, accuracy, summary_op),\n",
    "                                      feed_dict={x: mnist.train.images[:5000,:], y: mnist.train.labels[:5000]})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            summary_writer.add_summary(summary, i)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Step {0}: accuracy is {1}'.format(i + 1, ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name scopes\n",
    "\n",
    "Although our previous example was functional, the graph that was created is very messy. We would like to separate the graphs into distinct components, and modern neural network architectures are often built by stacking similar components on top of each other. Tensorflow's mechanism to build these reusable components is the notion of scopes, which introduce name prefixes. They also group the operation as displayed in the tensorboard graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope_1/a:0\n",
      "scope_2/a:0\n"
     ]
    }
   ],
   "source": [
    "# A very simple example of using scopes\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope('scope_1'):\n",
    "        a1 = tf.get_variable('a', [10])\n",
    "    \n",
    "    with tf.variable_scope('scope_2'):\n",
    "        a2 = tf.get_variable('a', [10])\n",
    "    \n",
    "    print(a1.name)\n",
    "    print(a2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope_1/myvar:0\n",
      "scope_2/myvar:0\n"
     ]
    }
   ],
   "source": [
    "# We can also use scopes when calling functions\n",
    "# This is the main way we will use them, to be able to reuse\n",
    "# different blocks (that we may write as functions) in a single\n",
    "# model\n",
    "\n",
    "def create_variable():\n",
    "    return tf.get_variable('myvar', [10])\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope('scope_1'):\n",
    "        m1 = create_variable()\n",
    "    \n",
    "    with tf.variable_scope('scope_2'):\n",
    "        m2 = create_variable()\n",
    "    \n",
    "    print(m1.name)\n",
    "    print(m2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: accuracy is 0.13579998910427094\n",
      "Step 11: accuracy is 0.822199821472168\n",
      "Step 21: accuracy is 0.855999767780304\n",
      "Step 31: accuracy is 0.870999813079834\n",
      "Step 41: accuracy is 0.8807997703552246\n",
      "Step 51: accuracy is 0.8897998332977295\n",
      "Step 61: accuracy is 0.8949997425079346\n",
      "Step 71: accuracy is 0.8981997966766357\n",
      "Step 81: accuracy is 0.9007998108863831\n",
      "Step 91: accuracy is 0.9027997851371765\n"
     ]
    }
   ],
   "source": [
    "# Let's apply this to our example above!\n",
    "\n",
    "def compute_logits(x):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    W = tf.get_variable('W', shape=[784, 10])\n",
    "    b = tf.get_variable('b', shape=[10])\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Note: this function is implemented in tensorflow as\n",
    "# tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# We have included it here for illustration only, please don't use it.\n",
    "def compute_cross_entropy(logits, y):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    with tf.name_scope('accuracy'):\n",
    "        prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "        true_label = tf.argmax(y, 1, name='true_class')\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits = compute_logits(x)\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train_step = opt.minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Let's put the summaries below\n",
    "    \n",
    "    with tf.variable_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 28, 28, 1]))\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter('logs/example2', sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(100):\n",
    "            _, ac, summary = sess.run((train_step, accuracy, summary_op),\n",
    "                                      feed_dict={x: mnist.train.images[:5000,:], y: mnist.train.labels[:5000]})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            summary_writer.add_summary(summary, i)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Step {0}: accuracy is {1}'.format(i + 1, ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the training loop: monitored sessions\n",
    "\n",
    "So far, we have been handling running the training loop ourselves: feeding the new data, incrementing the step counter, etc. We have not implemented any functionality concerning saving and restoring our trained model yet. Fortunately, tensorflow provides a pre-made training loop, called `tf.MonitoredTrainingSession`. Let us use this instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into logs/example3\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.8642\n",
      "INFO:tensorflow:global_step/sec: 50.5714\n",
      "INFO:tensorflow:global_step/sec: 50.5815\n",
      "INFO:tensorflow:global_step/sec: 46.6678\n",
      "INFO:tensorflow:global_step/sec: 46.6618\n",
      "INFO:tensorflow:global_step/sec: 46.077\n",
      "INFO:tensorflow:global_step/sec: 45.4713\n",
      "INFO:tensorflow:global_step/sec: 48.8125\n",
      "INFO:tensorflow:global_step/sec: 47.7436\n",
      "INFO:tensorflow:global_step/sec: 46.5452\n",
      "INFO:tensorflow:Saving checkpoints for 101 into logs/example3\\model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# Let's apply this to our example above!\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits = compute_logits(x)\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    # We use the global step to keep track of how many\n",
    "    # steps we have. It integrates with all the other\n",
    "    # tensorflow logging functionality.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    \n",
    "    # This will automatically increment the global step.\n",
    "    train_step = opt.minimize(loss, global_step)\n",
    "    \n",
    "    # Let's put the summaries below\n",
    "    with tf.variable_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 28, 28, 1]))\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Note: the monitored training session will automatically save summaries and checkpoints for us!\n",
    "    # It also initializes the variables for us.\n",
    "    with tf.train.MonitoredTrainingSession(save_summaries_steps=10,\n",
    "                                           checkpoint_dir='logs/example3',\n",
    "                                           log_step_count_steps=10) as sess:\n",
    "        while not sess.should_stop():\n",
    "            # Could also print more info here as before.\n",
    "            _, step = sess.run((train_step, global_step),\n",
    "                               feed_dict={x: mnist.train.images[:5000,:], y: mnist.train.labels[:5000]})\n",
    "            if step > 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections\n",
    "\n",
    "How does the `tf.train.MonitoredTrainingSession` keep track of all the summaries we have defined? Or how does `tf.train.GradientDescentOptimizer` know which variables to take derivatives for? Tensorflow has a notion of collections for a graph, which is simply a list of operations in the graph. When creating some operations, you may add (or tensorflow may automatically add) them to some collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'x:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'y:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "# trainable variables\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.get_variable('x', [10])\n",
    "    y = tf.get_variable('y', [10])\n",
    "    z = tf.get_variable('z', [10], trainable=False)\n",
    "    \n",
    "    print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'mean_squared_error/value:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Losses\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.get_variable('x', [10])\n",
    "    y = tf.get_variable('y', [10])\n",
    "    \n",
    "    tf.losses.mean_squared_error(x, y)\n",
    "    \n",
    "    print(tf.get_collection(tf.GraphKeys.LOSSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a large number of predefined collections (or you can create your own), but only a few will have operations automatically added to them. You can always add them yourself to organize your models better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last words\n",
    "\n",
    "Tensorflow is still in rapid development, and best practices for tensorflow keep changing. The setup in this tutorial is more than adequate for all the problems we will be exploring in this course, but numerous other APIs exist in tensorflow. The currently recommended API for tensorflow is the [`tf.estimator`](https://www.tensorflow.org/get_started/estimator) API for learning combined with the [`tf.contrib.data`](https://www.tensorflow.org/programmers_guide/datasets) API for loading data &mdash; they are designed for more large scale projects than your homeworks but may be useful for some projects.\n",
    "\n",
    "Even more than usual, coding in tensorflow can be frustrating, and the errors can be opaque. However, it is one of the only tools we have today to create machine learning models that can achieve such high performance in these tasks. As always, google is your friend.\n",
    "\n",
    "Unlike some other machine learning techniques, deep learning can be extremely data and computation intensive. The homework examples will be simple enough to run on a simple laptop, but any serious project will require serious computational power and a GPU (K80 or equivalent). Fortunately, it is possible to access such resources in an affordable fashion on cloud platforms, such as [AWS](https://aws.amazon.com/ec2/pricing/) although they do require some technical skills to setup. In particular, Github offers a [student pack](https://education.github.com/pack) which includes a fair amount of AWS credit.\n",
    "\n",
    "Other online solutions exist that manage the IT aspects for you. They tend to be slightly more expensive than simply setting up your own machine on AWS, but are much simpler to use. For example, you may consider: [FloydHub](https://www.floydhub.com/), [Paperspace](https://www.paperspace.com/), [Neptune ML](https://neptune.ml/), [Tensorport](https://tensorport.com/), [Crestle](https://www.crestle.com/).\n",
    "\n",
    "Finally, deep learning is as much art as it is science. The main way to improve is simply practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
